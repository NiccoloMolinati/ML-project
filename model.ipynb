{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset \n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import transformers as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"neuralcatcher/hateful_memes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.text_df = df[[\"text\",\"label\"]].astype({\"text\":str, \"label\":float})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text_df[\"text\"][index]\n",
    "        label = self.text_df[\"label\"][index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids=False,\n",
    "            padding='longest', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].flatten(), \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"labels\": t.tensor(label, dtype=t.long)}\n",
    "        \n",
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "        self.image_df = df[[\"img\",\"label\"]]\n",
    "        self.image_df = df.astype({\"img\":str, \"label\":float})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_pth = os.path.join(self.img_dir, self.image_df[\"img\"])\n",
    "        img = Image.open(img_pth).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0)\n",
    "        label = self.image_df[\"label\"][index]\n",
    "\n",
    "        return img_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument unpacking (4076218601.py, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 54\u001b[1;36m\u001b[0m\n\u001b[1;33m    outputs = model(**toktext, labels)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument unpacking\n"
     ]
    }
   ],
   "source": [
    "class MonoConcatModel(nn.Module):\n",
    "    def __init__(self, text_model, img_model, tokenizer, text_dims, img_dims):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_model = text_model\n",
    "        self.img_model = img_model\n",
    "\n",
    "\n",
    "        # architecture\n",
    "        #ToxicBERT/HateBERT\n",
    "        self.textfc = nn.Linear(text_dims, 16)\n",
    "        self.imgfc = nn.Linear(img_dims, 16)\n",
    "        self.fusion = nn.Sequential(nn.Linear(32, 16), nn.Relu, nn.Linear(16, 2))\n",
    "\n",
    "    def forward(self, tokenized_text, processed_image):\n",
    "        #to be done\n",
    "        txt = tokenized_text\n",
    "        img = processed_image\n",
    "    \n",
    "    # def process_text(self, text):\n",
    "    #     #take da text n tokenize that shit, then put it in da model\n",
    "    #     model = self.text_model\n",
    "    #     tokenizer = tr.AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "    #     toktext = tokenizer(text)\n",
    "    #     return toktext\n",
    "\n",
    "    \n",
    "    # def process_image(self, image_path):\n",
    "        #idk resnet50?\n",
    "    #     model = self.img_model\n",
    "    #     transform_img = transforms.Compose([\n",
    "    #                 transforms.Resize((224, 224)),\n",
    "    #                 transforms.ToTensor(),\n",
    "    #                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #             ])\n",
    "    #     img = Image.open(image_path).convert(\"RGB\")\n",
    "    #    img_mtrx = transform_img(img).unsqueeze(0)\n",
    "    #     return img_mtrx\n",
    "    \n",
    "    def fine_tune_subpart(self, dataloader, input_type):\n",
    "        model = self.text_model if input_type == \"txt\" else self.img_model\n",
    "        model.train()\n",
    "\n",
    "        tot_loss = 0\n",
    "        correct_preds = 0\n",
    "\n",
    "\n",
    "        for e in range(3):\n",
    "            if input_type == \"txt\":\n",
    "                for batch in dataloader:\n",
    "                    toktext = batch[\"input_ids\"]\n",
    "                    mask = batch[\"attention_mask\"]\n",
    "                    labels = batch[\"labels\"]\n",
    "                    outputs = model(input_ids=toktext, attention_mask=mask, labels=labels)\n",
    "\n",
    "                    # loss= nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.AdamW(model.parameters, lr= 5e-5)\n",
    "                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    tot_loss += loss.item()\n",
    "\n",
    "                    pred = t.softmax(logits, 1)\n",
    "\n",
    "            elif input_type == \"img\":\n",
    "                for images, labels in dataloader:\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tot_loss += loss.item()\n",
    "                    \n",
    "                    _, pred = t.max(outputs.data, 1)\n",
    "            \n",
    "            else:\n",
    "                print (\"input_type must be either 'txt' or 'img'\")\n",
    "                \n",
    "            correct_preds = t.sum(pred == labels)\n",
    "            return \"loss:\", tot_loss/len(dataloader), \"accuracy:\", correct_preds/len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
