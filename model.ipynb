{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicco\\OneDrive\\Desktop\\DHDK\\2ndYear\\courses\\MLfH\\ex5\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset \n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 8500/8500 [00:00<00:00, 128339.59 examples/s]\n",
      "Generating validation split: 100%|██████████| 1040/1040 [00:00<00:00, 51985.18 examples/s]\n",
      "Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 120378.39 examples/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id            img  label  \\\n",
      "0  42953  img/42953.png      0   \n",
      "\n",
      "                                               text  \n",
      "0  its their character not their color that matters  \n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "with open(\"hateful_memes/train.jsonl\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "lines_dict = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines_dict)\n",
    "print(df.loc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files moved\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "'''\n",
    "imgs_referenced = df[\"img\"]\n",
    "imgs_referenced.astype(str)\n",
    "\n",
    "def select_ref_imgs(orig_loc):\n",
    "    orig_loc = str(orig_loc)\n",
    "    file_name = orig_loc[4:]\n",
    "   \n",
    "    try:\n",
    "        shutil.move(f\".\\hateful_memes\\{orig_loc}\", f\".\\hateful_memes\\imgs_present/{file_name}\")\n",
    "        print(f\"file {file_name} moved\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"file {file_name} not found\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"error:{e}\")\n",
    "\n",
    "def change_slash(string):\n",
    "    string = string.replace(\"/\",  \"\\ \" )\n",
    "    string = string.replace(\" \", \"\")\n",
    "    return string\n",
    "    '''\n",
    "\n",
    "# imgs_referenced = imgs_referenced.apply(change_slash)\n",
    "# imgs_referenced.apply(select_ref_imgs)\n",
    "\n",
    "print(\"files moved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files deleted\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# for file_name in os.listdir(\"./hateful_memes/img\"):\n",
    "    # file_path = os.path.join(\"./hateful_memes/img\", file_name)\n",
    "    # os.remove(file_path)\n",
    "    # print(f\"{file_path} deleted.\")\n",
    "    # pass\n",
    "print (\"files deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# shutil.rmtree(\"hateful_memes/img_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# os.rename(\"hateful_memes/imgs_present\", \"hateful_memes/img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files present\n"
     ]
    }
   ],
   "source": [
    "def check_imgs_exist(file_path):\n",
    "    file_path = str(file_path)\n",
    "    file_name = file_path[4:]\n",
    "    if os.path.isfile(f\"hateful_memes/{file_path}\"):\n",
    "        print(f\"{file_name} present\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "reference_df = df\n",
    "reference_df[\"img_exists\"] = reference_df[\"img\"].apply(check_imgs_exist)\n",
    "reference_df = reference_df[reference_df[\"img_exists\"]]\n",
    "reference_df = reference_df.drop(columns=[\"img_exists\"])\n",
    "reference_df = reference_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     id            img  label  \\\n",
      "0      0  42953  img/42953.png      0   \n",
      "1      2  13894  img/13894.png      0   \n",
      "2      4  82403  img/82403.png      0   \n",
      "3      5  16952  img/16952.png      0   \n",
      "4      6  76932  img/76932.png      0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1                           putting bows on your pet  \n",
      "2  everybody loves chocolate chip cookies, even h...  \n",
      "3           go sports! do the thing! win the points!  \n",
      "4     fine you're right. now can we fucking drop it?   6744\n"
     ]
    }
   ],
   "source": [
    "print(reference_df.head(), len(reference_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index     id            img  label  \\\n",
      "882   1112  14507  img/14507.png      1   \n",
      "\n",
      "                                                  text  \n",
      "882  when your history teacher starts talking about...  \n"
     ]
    }
   ],
   "source": [
    "print(reference_df.query(\"id == '14507'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = reference_df.astype({\"index\":int, \"id\":int, \"img\":str, \"label\":float, \"text\":str})\n",
    "data = reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.text_df = df[[\"text\",\"label\"]].astype({\"text\":str, \"label\":float})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text_df[\"text\"][index]\n",
    "        label = self.text_df[\"label\"][index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids=False,\n",
    "            padding='longest', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].flatten(), \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"labels\": t.tensor(label, dtype=t.long)}\n",
    "        \n",
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "        self.image_df = df[[\"img\",\"label\"]]\n",
    "        self.image_df = self.image_df.astype({\"img\":str, \"label\":float})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_pth = os.path.join(self.img_dir, self.image_df[\"img\"])\n",
    "        img = Image.open(img_pth).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0)\n",
    "        label = self.image_df[\"label\"][index]\n",
    "\n",
    "        return img_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument unpacking (4076218601.py, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 54\u001b[1;36m\u001b[0m\n\u001b[1;33m    outputs = model(**toktext, labels)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument unpacking\n"
     ]
    }
   ],
   "source": [
    "class MonoLateFusModel(nn.Module):\n",
    "    def __init__(self, text_model, img_model, tokenizer, text_dims, img_dims):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_model = text_model\n",
    "        self.img_model = img_model\n",
    "\n",
    "\n",
    "        # architecture\n",
    "        #ToxicBERT/HateBERT input\n",
    "        self.textfc = nn.Linear(text_dims, 16)\n",
    "        #ResNet input\n",
    "        self.imgfc = nn.Linear(img_dims, 16)\n",
    "        self.fusion = nn.Sequential(nn.Linear(32, 16), nn.Relu, nn.Linear(16, 2))\n",
    "\n",
    "    def forward(self, tokenized_text, processed_image):\n",
    "        #to be done\n",
    "        txt = tokenized_text\n",
    "        img = processed_image\n",
    "\n",
    "        txt_proc = self.text_model()\n",
    "        img_proc = self.img_model()\n",
    "        combined_outputs = t.cat([txt_proc, img_proc], 1)\n",
    "        outputs = self.fusion(combined_outputs)\n",
    "\n",
    "        return outputs\n",
    "    # def process_text(self, text):\n",
    "    #     #take da text n tokenize that shit, then put it in da model\n",
    "    #     model = self.text_model\n",
    "    #     tokenizer = tr.AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "    #     toktext = tokenizer(text)\n",
    "    #     return toktext\n",
    "\n",
    "    \n",
    "    # def process_image(self, image_path):\n",
    "        #idk resnet50?\n",
    "    #     model = self.img_model\n",
    "    #     transform_img = transforms.Compose([\n",
    "    #                 transforms.Resize((224, 224)),\n",
    "    #                 transforms.ToTensor(),\n",
    "    #                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #             ])\n",
    "    #     img = Image.open(image_path).convert(\"RGB\")\n",
    "    #    img_mtrx = transform_img(img).unsqueeze(0)\n",
    "    #     return img_mtrx\n",
    "    \n",
    "    def fine_tune_subpart(self, dataloader, input_type):\n",
    "        model = self.text_model if input_type == \"txt\" else self.img_model\n",
    "        model.train()\n",
    "\n",
    "        tot_loss = 0\n",
    "        correct_preds = 0\n",
    "\n",
    "\n",
    "        for e in range(3):\n",
    "            if input_type == \"txt\":\n",
    "                for batch in dataloader:\n",
    "                    toktext = batch[\"input_ids\"]\n",
    "                    mask = batch[\"attention_mask\"]\n",
    "                    labels = batch[\"labels\"]\n",
    "                    outputs = model(input_ids=toktext, attention_mask=mask, labels=labels)\n",
    "\n",
    "                    # loss= nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.AdamW(model.parameters, lr= 5e-5)\n",
    "                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    tot_loss += loss.item()\n",
    "\n",
    "                    pred = t.softmax(logits, 1)\n",
    "\n",
    "            elif input_type == \"img\":\n",
    "                for images, labels in dataloader:\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tot_loss += loss.item()\n",
    "                    \n",
    "                    _, pred = t.max(outputs.data, 1)\n",
    "            \n",
    "            else:\n",
    "                print (\"input_type must be either 'txt' or 'img'\")\n",
    "                \n",
    "            correct_preds = t.sum(pred == labels)\n",
    "            return \"loss:\", tot_loss/len(dataloader), \"accuracy:\", correct_preds/len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
