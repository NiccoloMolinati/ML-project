{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicco\\OneDrive\\Desktop\\DHDK\\2ndYear\\courses\\MLfH\\ex5\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset \n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "import json\n",
    "import shutil\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id            img  label  \\\n",
      "0  42953  img/42953.png      0   \n",
      "\n",
      "                                               text  \n",
      "0  its their character not their color that matters  \n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "\"\"\"lines = []\n",
    "with open(\"hateful_memes/train.jsonl\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "lines_dict = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines_dict)\n",
    "print(df.loc[[0]])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files moved\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "'''\n",
    "imgs_referenced = df[\"img\"]\n",
    "imgs_referenced.astype(str)\n",
    "\n",
    "def select_ref_imgs(orig_loc):\n",
    "    orig_loc = str(orig_loc)\n",
    "    file_name = orig_loc[4:]\n",
    "   \n",
    "    try:\n",
    "        shutil.move(f\".\\hateful_memes\\{orig_loc}\", f\".\\hateful_memes\\imgs_present/{file_name}\")\n",
    "        print(f\"file {file_name} moved\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"file {file_name} not found\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"error:{e}\")\n",
    "\n",
    "def change_slash(string):\n",
    "    string = string.replace(\"/\",  \"\\ \" )\n",
    "    string = string.replace(\" \", \"\")\n",
    "    return string\n",
    "    '''\n",
    "\n",
    "# imgs_referenced = imgs_referenced.apply(change_slash)\n",
    "# imgs_referenced.apply(select_ref_imgs)\n",
    "\n",
    "print(\"files moved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files deleted\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# for file_name in os.listdir(\"./hateful_memes/img\"):\n",
    "    # file_path = os.path.join(\"./hateful_memes/img\", file_name)\n",
    "    # os.remove(file_path)\n",
    "    # print(f\"{file_path} deleted.\")\n",
    "    # pass\n",
    "print (\"files deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# shutil.rmtree(\"hateful_memes/img_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# os.rename(\"hateful_memes/imgs_present\", \"hateful_memes/img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files present\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "\"\"\"def check_imgs_exist(file_path):\n",
    "    file_path = str(file_path)\n",
    "    file_name = file_path[4:]\n",
    "    if os.path.isfile(f\"hateful_memes/{file_path}\"):\n",
    "        # print(f\"{file_name} present\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "reference_df = df\n",
    "reference_df[\"img_exists\"] = reference_df[\"img\"].apply(check_imgs_exist)\n",
    "reference_df = reference_df[reference_df[\"img_exists\"]]\n",
    "reference_df = reference_df.drop(columns=[\"img_exists\"])\n",
    "reference_df = reference_df.reset_index()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     id            img  label  \\\n",
      "0      0  42953  img/42953.png      0   \n",
      "1      2  13894  img/13894.png      0   \n",
      "2      4  82403  img/82403.png      0   \n",
      "3      5  16952  img/16952.png      0   \n",
      "4      6  76932  img/76932.png      0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1                           putting bows on your pet  \n",
      "2  everybody loves chocolate chip cookies, even h...  \n",
      "3           go sports! do the thing! win the points!  \n",
      "4     fine you're right. now can we fucking drop it?   6744\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# print(reference_df.head(), len(reference_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index     id            img  label  \\\n",
      "882   1112  14507  img/14507.png      1   \n",
      "\n",
      "                                                  text  \n",
      "882  when your history teacher starts talking about...  \n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# print(reference_df.query(\"id == '14507'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# reference_df = reference_df.astype({\"index\":int, \"id\":int, \"img\":str, \"label\":float, \"text\":str})\n",
    "# data = reference_df\n",
    "\n",
    "# data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index     id            img  label  \\\n",
      "0           0      0  42953  img/42953.png    0.0   \n",
      "1           1      2  13894  img/13894.png    0.0   \n",
      "2           2      4  82403  img/82403.png    0.0   \n",
      "3           3      5  16952  img/16952.png    0.0   \n",
      "4           4      6  76932  img/76932.png    0.0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1                           putting bows on your pet  \n",
      "2  everybody loves chocolate chip cookies, even h...  \n",
      "3           go sports! do the thing! win the points!  \n",
      "4     fine you're right. now can we fucking drop it?  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.text= df[\"text\"].values\n",
    "        self.label = df[\"label\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids=False,\n",
    "            padding='longest', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].flatten(), \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"labels\": t.tensor(label, dtype=t.long)}\n",
    "        \n",
    "class DatasetImg (Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "        self.img= df[\"img\"].values\n",
    "        self.label = df[\"label\"].astype(int).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_pth = os.path.join(self.img_dir, self.img[index])\n",
    "        img = Image.open(img_pth).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0)\n",
    "        img_tensor = img_tensor.squeeze(1)\n",
    "        label = self.label[index]\n",
    "\n",
    "        return img_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data: 30% for fine-tuning of sub-models (of which, 20% for validation)\n",
    "hb_and_rs, rest = train_test_split(data, test_size= 0.7)\n",
    "fine_tuning_subm, validation_subm = train_test_split(hb_and_rs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tr.BertTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\", clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = DatasetTxt(fine_tuning_subm, tokenizer=tokenizer)\n",
    "text_ds_val = DatasetTxt(validation_subm, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_ds = DatasetImg(fine_tuning_subm, img_dir=\"hateful_memes/\")\n",
    "imag_ds_val = DatasetImg(validation_subm, img_dir=\"hateful_memes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_txt = DataLoader(text_ds, batch_size=16, shuffle=True, collate_fn=d_collator)\n",
    "train_loader_img = DataLoader(imag_ds, batch_size=16, shuffle=True)\n",
    "train_loader_txt_val = DataLoader(text_ds, batch_size=16, shuffle=True, collate_fn=d_collator)\n",
    "train_loader_img_val = DataLoader(imag_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoLateFusModel(nn.Module):\n",
    "    def __init__(self, text_model, img_model, classes):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "\n",
    "        in_features_img = img_model.fc.in_features\n",
    "        img_model.fc = nn.Linear(in_features_img, 2)\n",
    "        self.img_model = img_model\n",
    "\n",
    "\n",
    "        # architecture\n",
    "        #ToxicBERT/HateBERT input\n",
    "        self.textfc = nn.Linear(classes, 24)\n",
    "        #ResNet input\n",
    "        self.imgfc = nn.Linear(classes, 24)\n",
    "        self.mlp = nn.Sequential(nn.Linear(24, 16), nn.ReLU(), nn.Linear(16, 2))\n",
    "\n",
    "    def forward(self, tokenized_text, image):\n",
    "        #to be done\n",
    "        cloned_txt_model = copy.deepcopy(self.text_model)\n",
    "        cloned_txt_model = cloned_txt_model.to(device)\n",
    "        \n",
    "        cloned_img_model = copy.deepcopy(self.img_model)\n",
    "        cloned_img_model = cloned_img_model.to(device)\n",
    "\n",
    "        if \"labels\" in tokenized_text.keys(): #check if the input has labels (model in training)\n",
    "        \n",
    "            toktext = tokenized_text[\"input_ids\"]\n",
    "            mask = tokenized_text[\"attention_mask\"]\n",
    "            labels = tokenized_text[\"labels\"]\n",
    "\n",
    "            # txt_proc = self.textfc(self.text_model(tokenized_text))\n",
    "            # img_proc = self.imgfc(self.img_model(image))\n",
    "\n",
    "            logits_for_txt = cloned_txt_model(input_ids=toktext, attention_mask=mask, labels=labels).logits\n",
    "        \n",
    "        else: #otherwise, model in evaluation, labels not required\n",
    "            toktext = tokenized_text[\"input_ids\"]\n",
    "            mask = tokenized_text[\"attention_mask\"]\n",
    "\n",
    "            logits_for_txt = cloned_txt_model(input_ids=toktext, attention_mask=mask).logits\n",
    "        \n",
    "        img_t = cloned_img_model(image)\n",
    "\n",
    "        #print(f\"HateBERT output: {logits_for_txt.shape}\")\n",
    "        #print(f\"ResNet-18 output: {img_t.shape}\")\n",
    "\n",
    "        txt_proc = self.textfc(logits_for_txt)\n",
    "        img_proc = self.imgfc(img_t)\n",
    "\n",
    "        #print(f\"text data after mlp: {txt_proc.shape}\")\n",
    "        #print(f\"image data after mlp: {img_proc.shape}\")\n",
    "\n",
    "        txt_fft = t.fft.fft(txt_proc)\n",
    "        img_fft = t.fft.fft(img_proc)\n",
    "        #print(f\"txt_fft shape: {txt_fft.shape}\",f\"img_fft shape: {img_fft.shape}\")\n",
    "        fusion = t.real(txt_fft * img_fft)\n",
    "        #print(f\"fusion shape: {fusion.shape}\")\n",
    "        logits = self.mlp(fusion)\n",
    "        #print(f\"logits shape: {logits.shape}\")\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def fine_tune_subpart(self, dataloader_train, dataloader_val,  input_type):\n",
    "        model = self.text_model if input_type == \"txt\" else self.img_model\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        train_loss = 0\n",
    "        train_correct_preds = 0\n",
    "        train_tot_elements = 0\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct_preds = 0\n",
    "        val_tot_elements = 0\n",
    "\n",
    "\n",
    "\n",
    "        if input_type == \"txt\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"classifier\" not in name:\n",
    "                    param.requires_grad = False\n",
    "            optimizer_txt = optim.AdamW(model.parameters(), lr= 5e-5)\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer_txt, step_size=1, gamma=0.9)\n",
    "            \n",
    "        elif input_type == \"img\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"fc\" not in name:\n",
    "                    param.requires_grad = False\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer_img = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "        for e in range(5):\n",
    "            if input_type == \"txt\":\n",
    "                for batch in dataloader_train:\n",
    "                    toktext = batch[\"input_ids\"]\n",
    "                    mask = batch[\"attention_mask\"]\n",
    "                    labels = batch[\"labels\"]\n",
    "                    \n",
    "                    toktext, mask, labels = toktext.to(device), mask.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = model(input_ids=toktext, attention_mask=mask, labels=labels)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    optimizer_txt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_txt.step()\n",
    "                    scheduler.step()\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                    pred = t.argmax(logits, -1)\n",
    "                # print(f\"LOSS: {loss.item()}, PRED: {pred}\")\n",
    "                    train_correct_preds += t.sum(pred == labels)\n",
    "                    train_tot_elements += len(labels)\n",
    "\n",
    "                with t.no_grad: #validation\n",
    "                    for val_batch in dataloader_val:\n",
    "                        val_toktext = val_batch[\"input_ids\"]\n",
    "                        val_mask = val_batch[\"attention_mask\"]\n",
    "                        val_label = val_batch[\"labels\"]\n",
    "                        \n",
    "                        val_toktext, val_mask, val_label = val_toktext.to(device), val_mask.to(device), val_label.to(device)\n",
    "                        # print(text_data)\n",
    "\n",
    "                        val_outputs = model(input_ids=toktext, attention_mask=mask)\n",
    "                        #print (outputs)\n",
    "                        current_loss_val = val_outputs.loss\n",
    "\n",
    "                        val_loss += current_loss_val.item()\n",
    "                        _, val_pred = t.max(val_outputs.data, 1)\n",
    "\n",
    "                        val_correct_preds += t.sum(val_pred == val_label)\n",
    "                        val_tot_elements = len(val_label)\n",
    "                    \n",
    "\n",
    "            elif input_type == \"img\":\n",
    "                for images, labels in dataloader_train:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    images = images.squeeze(1)\n",
    "\n",
    "                    optimizer_img.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs.float(), labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_img.step()\n",
    "                    train_loss += loss.item()\n",
    "                    \n",
    "                    #tot_val += labels.size(0)\n",
    "                    \n",
    "                    _, pred = t.max(outputs.data, 1)\n",
    "                # print(f\"LOSS: {loss.item()}, PRED: {pred}\")\n",
    "                    train_correct_preds += t.sum(pred == labels)\n",
    "                    train_tot_elements += len(labels)\n",
    "                with t.no_grad:\n",
    "                    for val_images, val_labels in dataloader_val:\n",
    "                        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                         \n",
    "                        val_images = val_images.squeeze(1)\n",
    "\n",
    "                        val_outputs = model(images)\n",
    "\n",
    "                        current_loss_val = criterion(outputs.float(), val_labels)\n",
    "                        val_loss += current_loss_val.item()\n",
    "                        \n",
    "                        #tot_val += labels.size(0)\n",
    "                        \n",
    "                        _, val_pred = t.max(val_outputs.data, 1)\n",
    "                    # print(f\"LOSS: {loss.item()}, PRED: {pred}\")\n",
    "                        val_correct_preds += t.sum(val_pred == val_labels)\n",
    "                        val_tot_elements += len(val_labels)\n",
    "            \n",
    "            else:\n",
    "                print (\"input_type must be either 'txt' or 'img'\")\n",
    "                \n",
    "                \n",
    "            print(f\"batch {e+1}/5\", \n",
    "        f\"train loss: {train_loss/(len(dataloader_train)*(e+1)):.4f}, train accuracy: {100*train_correct_preds/train_tot_elements:.2f}\"\n",
    "        f\"val loss: {val_loss/(len(dataloader_val)*(e+1)):.4f}, val accuracy: {100*val_correct_preds.float()/val_tot_elements:.2f}\")\n",
    "            \n",
    "        # return f\"loss: {train_loss/(len(dataloader_train)*(e+1))}, accuracy: {100*train_correct_preds/tot_elements:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_txt = tr.AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\", num_labels = 2)\n",
    "model_img = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_lf_model = MonoLateFusModel(text_model=model_txt, img_model=model_img, classes=2)\n",
    "hmc_lf_model = hmc_lf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/5 loss: 0.7418, accuracy: 3.1496\n",
      "batch 2/5 loss: 0.7384, accuracy: 3.9370\n",
      "batch 3/5 loss: 0.7369, accuracy: 4.7244\n",
      "batch 4/5 loss: 0.7363, accuracy: 4.7244\n",
      "batch 5/5 loss: 0.7362, accuracy: 4.7244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'loss: 0.7361526332736954, accuracy: 4.7244'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmc_lf_model.fine_tune_subpart(train_loader_txt, train_loader_txt_val, \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/5 loss: 0.6794, accuracy: 3.94\n",
      "batch 2/5 loss: 0.6663, accuracy: 4.72\n",
      "batch 3/5 loss: 0.6613, accuracy: 3.94\n",
      "batch 4/5 loss: 0.6519, accuracy: 3.94\n",
      "batch 5/5 loss: 0.6466, accuracy: 3.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'loss: 0.6465948087023938, accuracy: 3.94'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmc_lf_model.fine_tune_subpart(train_loader_img, train_loader_img_val, \"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_hmc (Dataset):\n",
    "    def __init__(self, df, tokenizer, img_path):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_dir = img_path\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "        self.text= df[\"text\"].values\n",
    "        self.label = df[\"label\"].values\n",
    "        self.img = df[\"img\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.text[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids=False,\n",
    "            padding='longest', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "        \n",
    "        img_pth = os.path.join(self.img_dir, self.img[index])\n",
    "        img = Image.open(img_pth).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0)\n",
    "        img_tensor = img_tensor.squeeze(1)\n",
    "        label = self.label[index]\n",
    "\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].flatten(), \n",
    "                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"img_tensor\": img_tensor,\n",
    "                \"labels\": t.tensor(label, dtype=t.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% of the whole dataset for training the multimodal model. 10% for validation and 10% for testing\n",
    "train_hmc, val_and_test = train_test_split(rest, test_size=0.25)\n",
    "validation_hmc, test_hmc = train_test_split(val_and_test, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_hmc = dataset_hmc(df=train_hmc, tokenizer=tokenizer, img_path=\"hateful_memes/\")\n",
    "val_ds_hmc = dataset_hmc(df=validation_hmc, tokenizer=tokenizer, img_path=\"hateful_memes/\")\n",
    "test_ds_hmc = dataset_hmc(df=test_hmc, tokenizer=tokenizer, img_path=\"hateful_memes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_hmc = DataLoader(train_ds_hmc, batch_size=16, shuffle=True, collate_fn=d_collator)\n",
    "val_dataloader_hmc = DataLoader(val_ds_hmc, batch_size=16, shuffle=True, collate_fn=d_collator)\n",
    "test_dataloader_hmc = DataLoader(test_ds_hmc, batch_size=16, shuffle=True, collate_fn=d_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/5 loss: 0.6340, accuracy: 10.62\n",
      "batch 2/5 loss: 0.6332, accuracy: 42.69\n",
      "batch 3/5 loss: 0.6295, accuracy: 96.53\n",
      "batch 4/5 loss: 0.6269, accuracy: 172.35\n",
      "batch 5/5 loss: 0.6259, accuracy: 269.76\n"
     ]
    }
   ],
   "source": [
    "hmc_lf_model.img_model.train()\n",
    "for name, param in hmc_lf_model.img_model.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "hmc_lf_model.text_model.train()\n",
    "for name, param in hmc_lf_model.text_model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in hmc_lf_model.named_parameters():\n",
    "    if not param.requires_grad and param.grad is not None:\n",
    "        print(f\"Warning: {name} has a gradient despite being frozen.\")\n",
    "    #print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "hmc_lf_model.train()\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "correct_preds = 0\n",
    "total_elements = 0\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer_general = t.optim.AdamW(filter(lambda p: p.requires_grad, hmc_lf_model.parameters()), \n",
    "    lr=1e-5\n",
    ")\n",
    "# optimizer_img = optim.Adam(hmc_lf_model.img_model.fc.parameters(), lr=0.001)\n",
    "# optimizer_txt = optim.AdamW(hmc_lf_model.text_model.parameters(), lr= 5e-5)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer_txt, step_size=1, gamma=0.9)\n",
    "\n",
    "for e in range(5):\n",
    "    for batch in train_dataloader_hmc:\n",
    "        \n",
    "        text_data = {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"labels\": batch[\"labels\"]\n",
    "        }\n",
    "        img = batch[\"img_tensor\"]\n",
    "        img_squeezed = img.squeeze(1)\n",
    "        label = batch[\"labels\"]\n",
    "\n",
    "        text_data, img_squeezed, label = text_data.to(device), img_squeezed.to(device), label.to(device)\n",
    "        # print(text_data)\n",
    "\n",
    "        outputs = hmc_lf_model(text_data,img_squeezed)\n",
    "        #print (outputs)\n",
    "        loss = loss_function(outputs, label)\n",
    "\n",
    "        optimizer_general.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_general.step()\n",
    "        #print(f\"loss: {loss.item()}\")\n",
    "        total_loss += loss.item()\n",
    "        _, pred = t.max(outputs.data, 1)\n",
    "\n",
    "        correct_preds += t.sum(pred == label)\n",
    "        total_elements += len(label)\n",
    "\n",
    "    print(f\"batch {e+1}/5\", \n",
    "        f\"loss: {total_loss/(len(train_dataloader_hmc)*(e+1)):.4f}, accuracy: {100*correct_preds/total_elements:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6084, accuracy: 10.97\n"
     ]
    }
   ],
   "source": [
    "hmc_lf_model.text_model.eval()\n",
    "hmc_lf_model.img_model.eval()\n",
    "hmc_lf_model.eval()\n",
    "\n",
    "\n",
    "val_loss = 0\n",
    "val_correct = 0\n",
    "val_total_el = 0\n",
    "for ve in range(5):\n",
    "    with t.no_grad():\n",
    "\n",
    "        for batch in val_dataloader_hmc:\n",
    "            \n",
    "            text_data_val = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"]\n",
    "            }\n",
    "            img_val = batch[\"img_tensor\"]\n",
    "            img_val = img_val.squeeze(1)\n",
    "            label_val = batch[\"labels\"]\n",
    "\n",
    "            text_data_val, img_val, label_val = text_data_val.to(device), img_val.to(device), label_val.to(device)\n",
    "            # print(text_data)\n",
    "            val_outputs = hmc_lf_model(text_data_val,img_val)\n",
    "            #print (outputs)\n",
    "            loss_val = loss_function(val_outputs, label_val)\n",
    "\n",
    "            val_loss += loss_val.item()\n",
    "            _, val_pred = t.max(val_outputs.data, 1)\n",
    "\n",
    "            # print(f\"predictions shape: {val_pred.shape}\", f\"labels shape: {label_val.shape}\")\n",
    "\n",
    "            val_correct += t.sum(val_pred == label_val)\n",
    "\n",
    "        print(f\"batch {ve+1}/5\",\n",
    "              f\"loss: {val_loss/(len(val_dataloader_hmc)*(ve+1)):.4f}, accuracy: {100*val_correct.float()/val_total_el:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
