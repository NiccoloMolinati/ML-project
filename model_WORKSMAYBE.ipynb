{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset \n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id            img  label  \\\n",
      "0  42953  img/42953.png      0   \n",
      "\n",
      "                                               text  \n",
      "0  its their character not their color that matters  \n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "\"\"\"lines = []\n",
    "with open(\"hateful_memes/train.jsonl\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "lines_dict = [json.loads(line) for line in lines]\n",
    "\n",
    "df = pd.DataFrame(lines_dict)\n",
    "print(df.loc[[0]])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files moved\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "'''\n",
    "imgs_referenced = df[\"img\"]\n",
    "imgs_referenced.astype(str)\n",
    "\n",
    "def select_ref_imgs(orig_loc):\n",
    "    orig_loc = str(orig_loc)\n",
    "    file_name = orig_loc[4:]\n",
    "   \n",
    "    try:\n",
    "        shutil.move(f\".\\hateful_memes\\{orig_loc}\", f\".\\hateful_memes\\imgs_present/{file_name}\")\n",
    "        print(f\"file {file_name} moved\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"file {file_name} not found\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"error:{e}\")\n",
    "\n",
    "def change_slash(string):\n",
    "    string = string.replace(\"/\",  \"\\ \" )\n",
    "    string = string.replace(\" \", \"\")\n",
    "    return string\n",
    "    '''\n",
    "\n",
    "# imgs_referenced = imgs_referenced.apply(change_slash)\n",
    "# imgs_referenced.apply(select_ref_imgs)\n",
    "\n",
    "print(\"files moved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files deleted\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# for file_name in os.listdir(\"./hateful_memes/img\"):\n",
    "    # file_path = os.path.join(\"./hateful_memes/img\", file_name)\n",
    "    # os.remove(file_path)\n",
    "    # print(f\"{file_path} deleted.\")\n",
    "    # pass\n",
    "print (\"files deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# shutil.rmtree(\"hateful_memes/img_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# os.rename(\"hateful_memes/imgs_present\", \"hateful_memes/img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files present\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "\"\"\"def check_imgs_exist(file_path):\n",
    "    file_path = str(file_path)\n",
    "    file_name = file_path[4:]\n",
    "    if os.path.isfile(f\"hateful_memes/{file_path}\"):\n",
    "        # print(f\"{file_name} present\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "reference_df = df\n",
    "reference_df[\"img_exists\"] = reference_df[\"img\"].apply(check_imgs_exist)\n",
    "reference_df = reference_df[reference_df[\"img_exists\"]]\n",
    "reference_df = reference_df.drop(columns=[\"img_exists\"])\n",
    "reference_df = reference_df.reset_index()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     id            img  label  \\\n",
      "0      0  42953  img/42953.png      0   \n",
      "1      2  13894  img/13894.png      0   \n",
      "2      4  82403  img/82403.png      0   \n",
      "3      5  16952  img/16952.png      0   \n",
      "4      6  76932  img/76932.png      0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1                           putting bows on your pet  \n",
      "2  everybody loves chocolate chip cookies, even h...  \n",
      "3           go sports! do the thing! win the points!  \n",
      "4     fine you're right. now can we fucking drop it?   6744\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# print(reference_df.head(), len(reference_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index     id            img  label  \\\n",
      "882   1112  14507  img/14507.png      1   \n",
      "\n",
      "                                                  text  \n",
      "882  when your history teacher starts talking about...  \n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# print(reference_df.query(\"id == '14507'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN CELL!\n",
    "\n",
    "# reference_df = reference_df.astype({\"index\":int, \"id\":int, \"img\":str, \"label\":float, \"text\":str})\n",
    "# data = reference_df\n",
    "\n",
    "# data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index     id            img  label  \\\n",
      "0           0      0  42953  img/42953.png    0.0   \n",
      "1           1      2  13894  img/13894.png    0.0   \n",
      "2           2      4  82403  img/82403.png    0.0   \n",
      "3           3      5  16952  img/16952.png    0.0   \n",
      "4           4      6  76932  img/76932.png    0.0   \n",
      "\n",
      "                                                text  \n",
      "0   its their character not their color that matters  \n",
      "1                           putting bows on your pet  \n",
      "2  everybody loves chocolate chip cookies, even h...  \n",
      "3           go sports! do the thing! win the points!  \n",
      "4     fine you're right. now can we fucking drop it?  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTxt (Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.text= df[\"text\"].values\n",
    "        self.label = df[\"label\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, return_token_type_ids=False,\n",
    "            padding='longest', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].flatten(), \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "                \"labels\": t.tensor(label, dtype=t.long)}\n",
    "        \n",
    "class DatasetImg (Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "        self.img= df[\"img\"].values\n",
    "        self.label = df[\"label\"].astype(int).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_pth = os.path.join(self.img_dir, self.img[index])\n",
    "        img = Image.open(img_pth).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0)\n",
    "        img_tensor = img_tensor.squeeze(1)\n",
    "        label = self.label[index]\n",
    "\n",
    "        return img_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data 50%\n",
    "train_data, rest = train_test_split(data, test_size= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tr.BertTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\", clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = DatasetTxt(train_data, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_ds = DatasetImg(train_data, img_dir=\"hateful_memes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_txt = DataLoader(text_ds, batch_size=16, shuffle=True, collate_fn=d_collator)\n",
    "train_loader_img = DataLoader(imag_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoLateFusModel(nn.Module):\n",
    "    def __init__(self, text_model, img_model, text_dims, img_dims):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.img_model = img_model\n",
    "\n",
    "\n",
    "        # architecture\n",
    "        #ToxicBERT/HateBERT input\n",
    "        self.textfc = nn.Linear(text_dims, 128)\n",
    "        #ResNet input\n",
    "        self.imgfc = nn.Linear(img_dims, 128)\n",
    "        self.mlp = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 2))\n",
    "\n",
    "    def forward(self, tokenized_text, image):\n",
    "        #to be done\n",
    "\n",
    "        txt_proc = self.textfc(self.text_model(tokenized_text))\n",
    "        img_proc = self.imgfc(self.img_model(image))\n",
    "\n",
    "        txt_fft = t.fft.fft(txt_proc)\n",
    "        img_fft = t.fft.fft(img_proc)\n",
    "\n",
    "        fusion = t.real(txt_fft * img_fft)\n",
    "        logits = self.mlp(fusion)\n",
    "        # combined_outputs = t.cat([txt_proc, img_proc], 1)\n",
    "        # outputs = self.fusion(combined_outputs)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def fine_tune_subpart(self, dataloader, input_type):\n",
    "        model = self.text_model if input_type == \"txt\" else self.img_model\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        tot_loss = 0\n",
    "        correct_preds = 0\n",
    "        if input_type == \"txt\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"classifier\" not in name:\n",
    "                    param.requires_grad = False\n",
    "            optimizer_txt = optim.AdamW(model.parameters(), lr= 5e-5)\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer_txt, step_size=1, gamma=0.9)\n",
    "        elif input_type == \"img\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"fc\" not in name:\n",
    "                    param.requires_grad = False\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer_img = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "        for e in range(3):\n",
    "            if input_type == \"txt\":\n",
    "                for batch in dataloader:\n",
    "                    toktext = batch[\"input_ids\"]\n",
    "                    mask = batch[\"attention_mask\"]\n",
    "                    labels = batch[\"labels\"]\n",
    "\n",
    "\n",
    "                    outputs = model(input_ids=toktext, attention_mask=mask, labels=labels)\n",
    "\n",
    "                    # loss= nn.CrossEntropyLoss()\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    optimizer_txt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_txt.step()\n",
    "                    scheduler.step()\n",
    "                    tot_loss += loss.item()\n",
    "\n",
    "                    pred = t.argmax(logits, -1)\n",
    "                    \n",
    "\n",
    "            elif input_type == \"img\":\n",
    "                for images, labels in dataloader:\n",
    "\n",
    "                    images = images.squeeze(1)\n",
    "\n",
    "                    optimizer_img.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs.float(), labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_img.step()\n",
    "                    tot_loss += loss.item()\n",
    "                    \n",
    "                    _, pred = t.max(outputs.data, 1)\n",
    "            \n",
    "            else:\n",
    "                print (\"input_type must be either 'txt' or 'img'\")\n",
    "                \n",
    "            correct_preds = t.sum(pred == labels)\n",
    "            print(f\"batch {e+1}/3\", \n",
    "                  f\"loss: {tot_loss/(len(dataloader)*(e+1)):.4f}, accuracy: {correct_preds/(len(dataloader)*(e+1)):.4f}\")\n",
    "        return \"loss:\", tot_loss/len(dataloader), \"accuracy:\", correct_preds/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_txt = tr.AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\", num_labels = 2)\n",
    "model_img = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_lf_model = MonoLateFusModel(text_model=model_txt, img_model=model_img, text_dims=768, img_dims=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1/3 loss: 0.6901544030243751, accuracy: 0.033175356686115265\n",
      "batch2/3 loss: 1.3740130353595408, accuracy: 0.028436018154025078\n",
      "batch3/3 loss: 2.061435442329583, accuracy: 0.0379146933555603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('loss:', 2.061435442329583, 'accuracy:', tensor(0.0379))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmc_lf_model.fine_tune_subpart(train_loader_txt, \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/3 loss: 1.2518, accuracy: 0.0284\n",
      "batch 2/3 loss: 0.9980, accuracy: 0.0213\n",
      "batch 3/3 loss: 0.8919, accuracy: 0.0142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('loss:', 2.6755912041212144, 'accuracy:', tensor(0.0427))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmc_lf_model.fine_tune_subpart(train_loader_img, \"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(105879, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.3, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.3, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hmc_lf_model.text_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
